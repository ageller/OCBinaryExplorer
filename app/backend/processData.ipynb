{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef1abd8",
   "metadata": {},
   "source": [
    "# Process the data from Anna for the web\n",
    "\n",
    "I will construct an sqlite database files for each cluster.\n",
    "\n",
    "For now, I am only taking the members.  And I am planning to create a different database file for each cluster.  (Currently the sample here for NGC 6819 is ~1GB in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "#from zipfile import ZipFile\n",
    "import tarfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cluster(cluster_name, prefix = '', \n",
    "                    run_cluster_res = True,\n",
    "                    run_cluster_summary = True,\n",
    "                    run_cluster_ms = True,\n",
    "                    run_sample_mass_files = True,\n",
    "                    ):\n",
    "    print('processing files for : ', cluster_name)\n",
    "    # create an sqlite file (or connect to existing file)\n",
    "    conn = sqlite3.connect(os.path.join('sqlite', prefix, cluster_name + '.db'))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    #with ZipFile(os.path.join('rawData', cluster_name + '.zip'), 'r') as zip_ref:\n",
    "    with tarfile.open(os.path.join('rawData', prefix, cluster_name + '.tar.gz'), 'r:gz') as tar_ref:\n",
    "\n",
    "        #file_list = zip_ref.namelist()\n",
    "        file_list = tar_ref.getnames()\n",
    "\n",
    "        if (run_cluster_res):\n",
    "            # read in and process the cluster .res file\n",
    "            #res_file = os.path.join(cluster_name, cluster_name + '.res')\n",
    "            res_file = os.path.join('.', cluster_name + '.res')\n",
    "            if res_file in file_list:\n",
    "                #with zip_ref.open(res_file) as f:\n",
    "                with tar_ref.extractfile(tar_ref.getmember(res_file)) as f:\n",
    "                    print(res_file)\n",
    "                    singlePop_res = pd.read_csv(f, delimiter = '\\s+')\n",
    "                    singlePop_res.insert(loc = 0, column = 'iteration', value = singlePop_res.index + 1)\n",
    "                    singlePop_res.to_sql('cluster_posterior', conn, if_exists = 'replace', index = False)\n",
    "            else:\n",
    "                print('ERROR! did not find', res_file)\n",
    "\n",
    "\n",
    "        if (run_cluster_summary or run_sample_mass_files):\n",
    "            # read in and process the cluster summary file\n",
    "            #sum_file = os.path.join(cluster_name, cluster_name + '.df')\n",
    "            sum_file = os.path.join('.', cluster_name + '.df')\n",
    "            if sum_file in file_list:\n",
    "                #with zip_ref.open(sum_file) as f:\n",
    "                with tar_ref.extractfile(tar_ref.getmember(sum_file)) as f:\n",
    "                    print(sum_file)\n",
    "                    stars_summary = pd.read_csv(f, delimiter = ' ')\n",
    "                    stars_summary.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "                    # looks like sqlite can't handle these as long ints\n",
    "                    stars_summary['source_id'] = stars_summary['source_id'].astype(str)\n",
    "                    # take only the members to reduce the file size?\n",
    "                    stars_summary_members = stars_summary.loc[stars_summary['member']]\n",
    "                    stars_summary_members.to_sql('stars_summary', conn, if_exists = 'replace', index = False)\n",
    "            else:\n",
    "                print('ERROR! did not find', sum_file)\n",
    "\n",
    "        if (run_cluster_ms):\n",
    "            # read in and process the .ms file\n",
    "            #ms_file = os.path.join(cluster_name, cluster_name + '_dir.ms')\n",
    "            ms_file = os.path.join('.', cluster_name + '.ms')\n",
    "            if ms_file in file_list:\n",
    "                #with zip_ref.open(ms_file) as f:\n",
    "                with tar_ref.extractfile(tar_ref.getmember(ms_file)) as f:\n",
    "                    print(ms_file)\n",
    "                    ms_df = pd.read_csv(f, delimiter = '\\s+')\n",
    "                    # fix the column names because sqlite is case insensitive\n",
    "                    ms_df.rename(columns = {'u':'u_SDSS', 'g':'g_SDSS', 'r':'r_SDSS', 'i':'i_SDSS', 'z':'z_SDSS'}, inplace = True)\n",
    "                    ms_df.to_sql('parsec_isochrone', conn, if_exists = 'replace', index = False)\n",
    "            else:\n",
    "                print('ERROR! did not find', ms_file)\n",
    "\n",
    "        if (run_sample_mass_files):\n",
    "            # parse through the sampleMass files\n",
    "            # get all the files\n",
    "            # iterate through the files to read them all in, and create tables for each star\n",
    "            print('sampleMass output ... ')\n",
    "            memIDs = stars_summary_members['source_id'].to_numpy()\n",
    "            have_sample_mass = np.array([False for x in memIDs])\n",
    "            #sample_mass_files = [x for x in file_list if ('sampleMass.out' in x and '__MACOSX' not in x)]\n",
    "            sample_mass_files = [x for x in file_list if ('samplemass.out' in x and '__MACOSX' not in x)]\n",
    "            for i, smf in enumerate(sample_mass_files):\n",
    "                #with zip_ref.open(smf) as f:\n",
    "                with tar_ref.extractfile(tar_ref.getmember(smf)) as f:\n",
    "                    print(f'{i} {(i+1)/len(sample_mass_files)*100:.2f} {smf}' )\n",
    "\n",
    "                    df = pd.read_csv(f, delimiter = '\\s+', quoting = csv.QUOTE_NONE)\n",
    "                    df.rename(columns = {'starId':'source_id'}, inplace = True)\n",
    "                    df['source_id'] = df['source_id'].astype(str)\n",
    "                    try:\n",
    "                        df['source_id'] = df['source_id'].str.replace('\"', '', regex = True)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # get the unique IDs\n",
    "                    uID = df['source_id'].unique()\n",
    "\n",
    "                    for iden in uID:\n",
    "                        # include only the members(?)\n",
    "                        #if (np.int64(iden) in memIDs):\n",
    "                        if (iden in memIDs):\n",
    "                            # j = np.where(memIDs == np.int64(iden))[0]\n",
    "                            j = np.where(memIDs == iden)[0]\n",
    "                            have_sample_mass[j] = True\n",
    "                            foo = df.loc[df['source_id'] == iden].copy()\n",
    "                            foo.drop('source_id', axis = 1, inplace = True)\n",
    "                            foo.to_sql('posterior_for_id_' + iden, conn, if_exists = 'replace', index = False)\n",
    "                    \n",
    "            found = np.where(have_sample_mass == True)[0]\n",
    "            missing = np.where(have_sample_mass == False)[0]\n",
    "            print('# of found sampleMass files : ', len(found))\n",
    "            print('# of missing sampleMass files : ', len(missing))\n",
    "\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b64ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the summary file\n",
    "\n",
    "cluster_params = pd.read_csv(os.path.join('rawData','paper2','cluster_params.csv'))\n",
    "cluster_params.columns = cluster_params.columns.str.replace(r\"[()]\", \"\")\n",
    "cluster_params.columns = cluster_params.columns.str.replace(\" \", \"_\")\n",
    "cluster_params.sort_values(by='Cluster', inplace = True)\n",
    "\n",
    "\n",
    "# add in NGC 6791 from the previous paper for the website, but not for zenodo\n",
    "# from paper 1 we had completeness-corrected values, which I will drop here (with \"_c\")\n",
    "# because of that, we had \"_i\" on some columns (for incomplete).  These are now our standard values, so I will remove the \"_i\".\n",
    "cluster_params_paper1 = pd.read_csv(os.path.join('rawData','paper1','cluster_params.csv'))\n",
    "cluster_params_paper1.columns = cluster_params_paper1.columns.str.replace(r\"[()]\", \"\")\n",
    "cluster_params_paper1.columns = cluster_params_paper1.columns.str.replace(\" \", \"_\")\n",
    "cluster_6791 = cluster_params_paper1.loc[cluster_params_paper1['Cluster'] == 'NGC_6791'].reset_index(drop=True)\n",
    "cluster_6791 = cluster_6791.loc[:,~cluster_6791.columns.str.endswith('_c')]\n",
    "cluster_6791 = cluster_6791.loc[:,~cluster_6791.columns.str.contains('_c_')]\n",
    "cluster_6791.columns = cluster_6791.columns.str.rstrip('_i') \n",
    "foo = cluster_6791['Center_RA_hr'].str.split()[0]\n",
    "cluster_6791.loc[0,'Center_RA_hr'] = float(foo[0]) + float(foo[1])/60 + float(foo[2])/3600\n",
    "foo = cluster_6791['Center_Dec_deg'].str.split()[0]\n",
    "cluster_6791.loc[0,'Center_Dec_deg'] = float(foo[0]) + float(foo[1])/60 + float(foo[2])/3600\n",
    "cluster_params_website = pd.concat([cluster_params, cluster_6791], ignore_index=True)\n",
    "\n",
    "cluster_params_website.sort_values(by='Cluster', inplace = True)\n",
    "\n",
    "cluster_params_website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c4a95",
   "metadata": {},
   "source": [
    "## NOTE : the NGC 6791 data is missing some columns because Paper 1 differs slightly from Paper 2... I should note this in the README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ce821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sql (for zenodo)\n",
    "conn = sqlite3.connect(os.path.join('sqlite', 'paper2', 'cluster_summary.db'))\n",
    "cursor = conn.cursor()\n",
    "cluster_params.to_sql('cluster_parameters', conn, if_exists = 'replace', index = False)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sql (for website)\n",
    "conn = sqlite3.connect(os.path.join('sqlite', 'paper2', 'cluster_summary_website.db'))\n",
    "cursor = conn.cursor()\n",
    "cluster_params_website.to_sql('cluster_parameters', conn, if_exists = 'replace', index = False)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_params.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the HDBSCAN file\n",
    "# SHOULD I CHANGE no_points to num_objects or something? (wait for Anna's README info)\n",
    "hdbscan_params = pd.read_csv(os.path.join('rawData','paper2','HDBSCAN_params.txt'), sep = '\\s+')\n",
    "hdbscan_params = hdbscan_params.rename(columns={'Min_size': 'min_size', 'Radius': 'radius', 'Group': 'group_number', 'no_points':'n_objects'})\n",
    "hdbscan_params.sort_values(by='Cluster', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea021b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = sqlite3.connect(os.path.join('sqlite', 'paper2', 'cluster_summary.db'))\n",
    "#conn = sqlite3.connect(os.path.join('sqlite', 'paper2', 'hdbscan_cluster_params.db'))\n",
    "cursor = conn.cursor()\n",
    "hdbscan_params.to_sql('hdbscan_cluster_parameters', conn, if_exists = 'replace', index = False)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(os.path.join('sqlite', 'paper2', 'cluster_summary_website.db'))\n",
    "cursor = conn.cursor()\n",
    "hdbscan_params.to_sql('hdbscan_cluster_parameters', conn, if_exists = 'replace', index = False)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper 1\n",
    "# clusters_to_process = ['NGC_6791','NGC_188','NGC_2682','NGC_6819','NGC_7789','NGC_2168']\n",
    "\n",
    "# paper 2\n",
    "# clusters_to_process = cluster_params['Cluster']\n",
    "\n",
    "\n",
    "clusters_to_process = [\n",
    "#    \"Berkeley_32\",\n",
    "#    \"Berkeley_39\",\n",
    "#    \"Collinder_394\",\n",
    "#    \"Haffner_22\",\n",
    "#    \"Melotte_71\",\n",
    "#    \"NGC_1245\",\n",
    "#    \"NGC_1664\",\n",
    "#    \"NGC_1817\",\n",
    "#    \"NGC_188\",\n",
    "#    \"NGC_1912\",\n",
    "#    \"NGC_2099\",\n",
    "#    \"NGC_2168\",\n",
    "#    \"NGC_2281\",\n",
    "#    \"NGC_2287\",\n",
    "#    \"NGC_2301\",\n",
    "#    \"NGC_2323\",\n",
    "#    \"NGC_2355\",\n",
    "#    \"NGC_2360\",\n",
    "#    \"NGC_2420\",\n",
    "#    \"NGC_2423\",\n",
    "#    \"NGC_2437\",\n",
    "#    \"NGC_2447\",\n",
    "#    \"NGC_2506\",\n",
    "#    \"NGC_2539\",\n",
    "#    \"NGC_2548\",\n",
    "#    \"NGC_2627\",\n",
    "#    \"NGC_2682\",\n",
    "#    \"NGC_6819\",\n",
    "#    \"NGC_6940\",\n",
    "#    \"NGC_7209\",\n",
    "#    \"NGC_7243\",\n",
    "#    \"NGC_7789\",\n",
    "#    \"Ruprecht_171\",\n",
    "#    \"Tombaugh_1\",\n",
    "#    \"Trumpler_3\",\n",
    "]\n",
    "\n",
    "for c in clusters_to_process:\n",
    "    process_cluster(c, prefix = 'paper2')#,                   \n",
    "                    # run_cluster_res = False,\n",
    "                    # run_cluster_summary = False,\n",
    "                    # run_cluster_ms = False,\n",
    "                    # run_sample_mass_files = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445d80b",
   "metadata": {},
   "source": [
    "# Testing accessing the data for the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'sqlite', 'paper2')\n",
    "conn = sqlite3.connect(os.path.join(data_dir, 'NGC_7789.db'))\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_clusters():\n",
    "    files = []\n",
    "    clusters = []\n",
    "    contents = os.listdir(data_dir)\n",
    "    for item in contents:\n",
    "        if os.path.isfile(os.path.join(data_dir, item)) and '.db' in item:\n",
    "            files.append(os.path.join(data_dir, item))\n",
    "            clusters.append(str.replace(item, '.db',''))\n",
    "\n",
    "    return files, clusters\n",
    "get_available_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eed8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_tables(cursor):\n",
    "    # get all the available tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tbls = cursor.fetchall()\n",
    "    tables = [t[0] for t in tbls]\n",
    "    return tables\n",
    "get_available_tables(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_columns(cursor, table_name):\n",
    "    # Execute the PRAGMA to get table information\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "\n",
    "    # Fetch all rows of the result\n",
    "    table_info = cursor.fetchall()\n",
    "\n",
    "    # return the column names\n",
    "    column_names = [row[1] for row in table_info]\n",
    "    return column_names\n",
    "\n",
    "get_available_columns(cursor, 'stars_summary')\n",
    "#get_available_columns(cursor, 'cluster_posterior')\n",
    "#get_available_columns(cursor, 'parsec_isochrone')\n",
    "#get_available_columns(cursor, 'posterior_for_id_1995014851613762432')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_data(cursor, table_name, column):\n",
    "    # select the data from the table\n",
    "    cursor.execute(f\"SELECT {column} FROM {table_name}\")\n",
    "    \n",
    "    # Fetch all the rows of that result\n",
    "    dd = cursor.fetchall()\n",
    "    \n",
    "    # return the data\n",
    "    data = [d[0] for d in dd]\n",
    "    return data\n",
    "get_column_data(cursor, 'posterior_for_id_1995014851613762432', 'membership')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be23f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'sqlite', 'paper2')\n",
    "conn = sqlite3.connect(os.path.join(data_dir, 'cluster_summary.db'))\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_tables(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d922559",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_columns(cursor, 'cluster_parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_data(cursor, 'cluster_parameters', 'Age_Gyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3531f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
