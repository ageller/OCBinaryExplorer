{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef1abd8",
   "metadata": {},
   "source": [
    "# Process the data from Anna for the web\n",
    "\n",
    "I will construct an sqlite database files for each cluster.\n",
    "\n",
    "For now, I am only taking the members.  And I am planning to create a different database file for each cluster.  (Currently the sample here for NGC 6819 is ~1GB in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cluster(cluster_name, run_sample_mass_files = True):\n",
    "    print('processing files for : ', cluster_name)\n",
    "    # create an sqlite file (or connect to existing file)\n",
    "    conn = sqlite3.connect(os.path.join('sqlite', cluster_name + '.db'))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    with ZipFile(os.path.join('rawData', cluster_name + '.zip'), 'r') as zip_ref:\n",
    "\n",
    "        file_list = zip_ref.namelist()\n",
    "\n",
    "        # read in and process the cluster .res file\n",
    "        res_file = os.path.join(cluster_name, cluster_name + '.res')\n",
    "        if res_file in file_list:\n",
    "            with zip_ref.open(res_file) as f:\n",
    "                print(res_file)\n",
    "                singlePop_res = pd.read_csv(f, delimiter = '\\s+')\n",
    "                singlePop_res.insert(loc = 0, column = 'iteration', value = singlePop_res.index + 1)\n",
    "                singlePop_res.to_sql('cluster_posterior', conn, if_exists = 'replace', index = False)\n",
    "        else:\n",
    "            print('ERROR! did not find', res_file)\n",
    "\n",
    "\n",
    "        # read in and process the cluster summary file\n",
    "        sum_file = os.path.join(cluster_name, cluster_name + '.df')\n",
    "        if sum_file in file_list:\n",
    "            with zip_ref.open(sum_file) as f:\n",
    "                print(sum_file)\n",
    "                stars_summary = pd.read_csv(f, delimiter = ' ')\n",
    "                stars_summary.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "                # take only the members to reduce the file size?\n",
    "                stars_summary_members = stars_summary.loc[stars_summary['member']]\n",
    "                stars_summary_members.to_sql('stars_summary', conn, if_exists = 'replace', index = False)\n",
    "        else:\n",
    "            print('ERROR! did not find', sum_file)\n",
    "\n",
    "        # read in and process the .ms file\n",
    "        ms_file = os.path.join(cluster_name, cluster_name + '_dir.ms')\n",
    "        if ms_file in file_list:\n",
    "            with zip_ref.open(ms_file) as f:\n",
    "                print(ms_file)\n",
    "                ms_df = pd.read_csv(f, delimiter = '\\s+')\n",
    "                # fix the column names because sqlite is case insensitive\n",
    "                ms_df.rename(columns = {'u':'u_SDSS', 'g':'g_SDSS', 'r':'r_SDSS', 'i':'i_SDSS', 'z':'z_SDSS'}, inplace = True)\n",
    "                ms_df.to_sql('parsec_isochrone', conn, if_exists = 'replace', index = False)\n",
    "        else:\n",
    "            print('ERROR! did not find', ms_file)\n",
    "\n",
    "        if (run_sample_mass_files):\n",
    "            # parse through the sampleMass files\n",
    "            # get all the files\n",
    "            # iterate through the files to read them all in, and create tables for each star\n",
    "            print('sampleMass output ... ')\n",
    "            memIDs = stars_summary_members['source_id'].to_numpy()\n",
    "            have_sample_mass = np.array([False for x in memIDs])\n",
    "            sample_mass_files = [x for x in file_list if ('sampleMass.out' in x and '__MACOSX' not in x)]\n",
    "            for i, smf in enumerate(sample_mass_files):\n",
    "                with zip_ref.open(smf) as f:\n",
    "                    print(f'{i} {(i+1)/len(sample_mass_files)*100:.2f} {smf}' )\n",
    "\n",
    "                    df = pd.read_csv(f, delimiter = '\\s+', quoting = csv.QUOTE_NONE)\n",
    "                    df.rename(columns = {'starId':'source_id'}, inplace = True)\n",
    "                    df['source_id'] = df['source_id'].str.replace('\"', '', regex = True)\n",
    "\n",
    "                    # get the unique IDs\n",
    "                    uID = df['source_id'].unique()\n",
    "\n",
    "                    for iden in uID:\n",
    "                        # include only the members(?)\n",
    "                        if (np.int64(iden) in memIDs):\n",
    "                            j = np.where(memIDs == np.int64(iden))[0]\n",
    "                            have_sample_mass[j] = True\n",
    "                            foo = df.loc[df['source_id'] == iden].copy()\n",
    "                            foo.drop('source_id', axis = 1, inplace = True)\n",
    "                            foo.to_sql('posterior_for_id_' + iden, conn, if_exists = 'replace', index = False)\n",
    "                    \n",
    "            found = np.where(have_sample_mass == True)[0]\n",
    "            missing = np.where(have_sample_mass == False)[0]\n",
    "            print('# of found sampleMass files : ', len(found))\n",
    "            print('# of missing sampleMass files : ', len(missing))\n",
    "\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dac62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the summary file\n",
    "conn = sqlite3.connect(os.path.join('sqlite',  'cluster_summary.db'))\n",
    "cursor = conn.cursor()\n",
    "cluster_params = pd.read_csv(os.path.join('rawData','cluster_params.csv'))\n",
    "cluster_params.to_sql('cluster_parameters', conn, if_exists = 'replace', index = False)\n",
    "cursor.close()\n",
    "conn.close()\n",
    "cluster_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_6791')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_188')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb66755",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_2682') #M_67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec847a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_6819') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_7789') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_2168') #M_35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
