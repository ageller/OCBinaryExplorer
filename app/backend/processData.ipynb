{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef1abd8",
   "metadata": {},
   "source": [
    "# Process the data for the web\n",
    "\n",
    "I will construct an sqlite database files for each cluster.\n",
    "\n",
    "For now, I am only taking the members.  And I am planning to create a different database file for each cluster.  (Currently the sample here for NGC 6819 is ~1GB in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cluster(cluster_name):\n",
    "\n",
    "    print('processing files for : ', cluster_name)\n",
    "    # create an sqlite file (or connect to existing file)\n",
    "    conn = sqlite3.connect(os.path.join('sqlite', cluster_name + '.db'))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # read in and process the cluster .res file\n",
    "    print(cluster_name + '.res')\n",
    "    singlePopRes = pd.read_csv(os.path.join('rawData', cluster_name, cluster_name + '.res'), delimiter = '\\s+')\n",
    "    singlePopRes.insert(loc = 0, column = 'iteration', value = singlePopRes.index + 1)\n",
    "    singlePopRes.to_sql('cluster_posterior', conn, if_exists = 'replace', index = False)\n",
    "\n",
    "    # read in and process the cluster\n",
    "    print(cluster_name + '.df')\n",
    "    starsSummary = pd.read_csv(os.path.join('rawData', cluster_name, cluster_name + '.df'), delimiter = ' ')\n",
    "    starsSummary.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    # take only the members to reduce the file size?\n",
    "    starsSummaryMembers = starsSummary.loc[starsSummary['member']]\n",
    "    starsSummaryMembers.to_sql('stars_summary', conn, if_exists = 'replace', index = False)\n",
    "\n",
    "    # parse through the sampleMass files\n",
    "    # get all the files\n",
    "    # iterate through the files to read them all in, and create tables for each star\n",
    "    print('sampleMass output ... ')\n",
    "    memIDs = starsSummaryMembers['source_id'].to_numpy()\n",
    "    haveSampleMass = np.array([False for x in memIDs])\n",
    "    directory = os.path.join('rawData', cluster_name, cluster_name + '_sampleMass')\n",
    "    files = os.listdir(directory)\n",
    "    for i, filename in enumerate(files):\n",
    "        if ('sampleMass.out' in filename):\n",
    "            f = os.path.join(directory, filename)\n",
    "            print(f'{i} {(i+1)/len(files)*100:.2f} {filename}' )\n",
    "\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "\n",
    "                df = pd.read_csv(f, delimiter = '\\s+', quoting = csv.QUOTE_NONE)\n",
    "                df.rename(columns = {'starId':'source_id'}, inplace = True)\n",
    "                df['source_id'] = df['source_id'].str.replace('\"', '', regex = True)\n",
    "\n",
    "                # get the unique IDs\n",
    "                uID = df['source_id'].unique()\n",
    "\n",
    "                for iden in uID:\n",
    "                    # include only the members(?)\n",
    "                    if (np.int64(iden) in memIDs):\n",
    "                        j = np.where(memIDs == np.int64(iden))[0]\n",
    "                        haveSampleMass[j] = True\n",
    "                        foo = df.loc[df['source_id'] == iden].copy()\n",
    "                        foo.drop('source_id', axis = 1, inplace = True)\n",
    "                        foo.to_sql('posterior_for_id_' + iden, conn, if_exists = 'replace', index = False)\n",
    "            \n",
    "    found = np.where(haveSampleMass == True)[0]\n",
    "    missing = np.where(haveSampleMass == False)[0]\n",
    "    print('# of found sampleMass files : ', len(found))\n",
    "    print('# of missing sampleMass files : ', len(missing))\n",
    "\n",
    "    # there's probably some linking that I could do, but for now I don't think it is necessary\n",
    "\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_6791')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_188')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb66755",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_2682') #M_67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec847a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_6819') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_7789') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cluster('NGC_2168') #M_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445d80b",
   "metadata": {},
   "source": [
    "# Testing accessing the data for the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'sqlite')\n",
    "conn = sqlite3.connect(os.path.join(data_dir, 'NGC_6819.db'))\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_clusters():\n",
    "    files = []\n",
    "    clusters = []\n",
    "    contents = os.listdir(data_dir)\n",
    "    for item in contents:\n",
    "        if os.path.isfile(os.path.join(data_dir, item)) and '.db' in item:\n",
    "            files.append(os.path.join(data_dir, item))\n",
    "            clusters.append(str.replace(item, '.db',''))\n",
    "\n",
    "    return files, clusters\n",
    "get_available_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eed8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_tables(cursor):\n",
    "    # get all the available tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tbls = cursor.fetchall()\n",
    "    tables = [t[0] for t in tbls]\n",
    "    return tables\n",
    "get_available_tables(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_columns(cursor, table_name):\n",
    "    # Execute the PRAGMA to get table information\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "\n",
    "    # Fetch all rows of the result\n",
    "    table_info = cursor.fetchall()\n",
    "\n",
    "    # return the column names\n",
    "    column_names = [row[1] for row in table_info]\n",
    "    return column_names\n",
    "\n",
    "#get_available_columns(cursor, 'stars_summary')\n",
    "get_available_columns(cursor, 'cluster_posterior')\n",
    "#get_available_columns(cursor, 'posterior_for_id_2076299826416672896')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_data(cursor, table_name, column):\n",
    "    # select the data from the table\n",
    "    cursor.execute(f\"SELECT {column} FROM {table_name}\")\n",
    "    \n",
    "    # Fetch all the rows of that result\n",
    "    dd = cursor.fetchall()\n",
    "    \n",
    "    # return the data\n",
    "    data = [d[0] for d in dd]\n",
    "    return data\n",
    "get_column_data(cursor, 'posterior_for_id_2076299826416672896', 'mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be23f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
